**Курс:** Проектирование систем машинного обучения

**Уровень:** 2 курс магистратуры

**Группа:** М8О-209СВ-24

**Студенты:** Макаров Глеб, Крижановская Анна

**Задание 16**

## 1. Введение и постановка задачи

16. **Прогнозирование предполагаемого времени прибытия (ETA):** Для службы доставки еды спроектируйте систему для прогнозирования ETA заказа. Сосредоточьтесь на включении данных о дорожном движении в реальном времени и нескольких этапах (приготовление на кухне, доставка).

**Бизнес-цели:**
1.  **Повышение удовлетворенности клиентов (Customer Satisfaction Score, CSAT):** Точные и надежные прогнозы времени доставки напрямую влияют на восприятие сервиса пользователем. Уменьшение количества опозданий и повышение предсказуемости услуги является ключевой бизнес-метрикой.
2.  **Оптимизация операционной эффективности:** Система позволяет ресторанам лучше планировать нагрузку на кухню, а логистическим координаторам — эффективнее распределять заказы между курьерами. Это ведет к снижению затрат на простои и сверхурочную работу, увеличению количества обрабатываемых заказов в час (Orders per Hour).
3.  **Увеличение вовлеченности и удержания пользователей (User Retention):** Удобный и предсказуемый сервис повышает лояльность. Клиенты, уверенные в своевременной доставке, с большей вероятностью совершают повторные заказы. Точный ETA можно использовать как инструмент коммуникации и управления ожиданиями, улучшая общий пользовательский опыт.

**Требования к системе:**
- Задержка (latency) ответа не должна превышать **138 мс**.
- Система должна обслуживать **1,417,129 активных пользователей в день (DAU)**, с пиковой нагрузкой в **19,576 запросов в секунду (RPS)**.
- Система должна быть масштабируемой и отказоустойчивой.

## 2. Задание

## Часть 1: Формулировка ML-задачи и выбор модели

### 1. Определение ML-задачи

Задачу можно сформулировать как задачу **регрессии с учителем (supervised regression)**. Для каждого заказа система должна предсказать непрерывное значение — общее время ETA (в минутах) от момента размещения заказа до доставки, включая этапы приготовления и доставки[citation:3].

-   **Входные данные (Features):**
    *   **Данные о заказе:** Идентификатор, время размещения, состав (тип и количество блюд), адрес доставки, стоимость.
    *   **Данные о ресторане:** Текущая загрузка кухни (очередь заказов), историческое среднее время приготовления для каждого блюда, географические координаты.
    *   **Данные о доставке:** Расстояние от ресторана до клиента (по дорогам), GPS-координаты ближайших свободных курьеров, тип транспорта курьера.
    *   **Контекстуальные данные в реальном времени:** Уровень дорожного движения на маршруте (из внешних API, например, Google Maps или Яндекс.Карты), текущие погодные условия, время суток, день недели (будний/выходной).

-   **Выходные данные (Prediction):** Предсказанное значение ETA в минутах. Рекомендуется также возвращать оценку уверенности модели (например, квантили прогноза) для информирования пользователя о возможном диапазоне времени.

-   **Целевая переменная (Ground Truth):** Фактическое время доставки, вычисленное как разница между временем фиксации доставки курьером в приложении и временем размещения заказа. Сбор этих "контрольных данных" (ground truth) является отдельной критически важной задачей для ML-системы[citation:3].

-   **Данные для обучения:** Исторические данные о заказах (миллионы записей), которые включают не только исходные данные (логи заказов, GPS-треки), но и рассчитанные признаки (фичи), а также размеченное целевое значение — фактическое ETA.

### 2. Выбор модели

Рассмотрим два подхода, соответствующие современным практикам построения ML-систем[citation:1][citation:9]:

1.  **Ансамбли на основе деревьев (XGBoost/LightGBM/CatBoost):**
    -   **Преимущества:** Высокая точность на табличных данных, хорошая интерпретируемость (важность признаков), устойчивость к выбросам, быстрое обучение и предсказание. Идеально подходят для структурированных признаков, извлекаемых из заказов, геоданных и исторической статистики. Поддерживают пропущенные значения.
    -   **Недостатки:** Требуют тщательной feature engineering для учета сложных временных зависимостей (например, эволюция пробки). Менее эффективны для сырых последовательностей данных (сырые GPS-точки).

2.  **Гибридная или специализированная нейросетевая архитектура (например, Transformer для временных рядов или графовые нейросети):**
    -   **Преимущества:** Могут напрямую обрабатывать сложные структуры данных (последовательности координат для построения маршрута, графы дорожной сети), автоматически извлекая из них признаки. Потенциально более высокая точность при наличии больших объемов данных.
    -   **Недостатки:** Значительно более высокая вычислительная сложность, длительное время обучения, требовательность к данным, сложность интерпретации и отладки, большая задержка инференса.

**Выбор:** Для данной задачи в качестве базовой и основной модели выбираем **градиентный бустинг (XGBoost/LightGBM)**. Этот выбор обоснован следующими факторами, критичными для промышленной системы:
*   **Соответствие требованию низкой задержки (<138 мс):** Инференс деревьев чрезвычайно быстр и детерминирован.
*   **Эффективность с табличными данными:** Большинство признаков в задаче (расстояние, время суток, исторические средние) являются табличными.
*   **Стабильность и предсказуемость:** Модели на основе деревьев менее склонны к катастрофическим сбоям при небольших изменениях входных данных по сравнению с нейросетями.
*   **Интерпретируемость:** Возможность анализа важности признаков критична для диагностики ошибок и объяснения модели бизнесу[citation:9].

Архитектура системы, однако, должна допускать возможность **постепенного усложнения модели** (например, добавление нейросетевого блока для обработки трафика) в будущем при сохранении надежности.

## Часть 2: Проектирование архитектуры

Ниже представлены диаграммы и детальное описание архитектуры системы, охватывающей полный жизненный цикл ML-модели: от сбора данных и обучения до развертывания, обслуживания и мониторинга[citation:1][citation:5].

### 1. Высокоуровневая архитектура системы

Данная диаграмма представляет собой общий взгляд на систему, показывая взаимодействие всех ключевых компонентов, источники данных и основные потоки информации[citation:7].

![Высокоуровневая архитектура системы](./img1.png)

**Пояснение и обоснование ключевых компонентов:**

*   **Источники данных (Client App, Restaurant System, Courier GPS, External APIs):** Это отправная точка для любого конвейера данных[citation:6]. Система должна интегрироваться с множеством разнородных источников: мобильные приложения пользователей и курьеров, внутренние системы ресторанов-партнеров, внешние сервисы карт и погоды. Использование асинхронных шаблонов обмена данными (например, через брокеры сообщений) позволяет развязать эти системы[citation:1].

*   **Брокер сообщений (Apache Kafka):** Выступает в роли центральной нервной системы для потоковых данных. Все события (новый заказ, обновление координат курьера, изменение статуса на кухне) публикуются в соответствующие топики Kafka. Это обеспечивает:
    1.  **Буферизацию пиковой нагрузки** (19,576 RPS) без потери данных.
    2.  **Возможность множественной обработки** одного и того же события разными подсистемами (например, инференс ETA и сбор данных для обучения).
    3.  **Повышение отказоустойчивости:** потребители данных (сервисы) могут отставать или перезапускаться без угрозы потери сообщений[citation:2][citation:10].

*   **Сервис инференса (Inference Service) в Kubernetes:** Ядро системы, отвечающее на запросы в реальном времени. Развертывание в Kubernetes обеспечивает горизонтальное масштабирование, управление жизненным циклом контейнеров и отказоустойчивость. Для соблюдения строгих требований к задержке используется быстрая модель (XGBoost в формате ONNX), кэширование и оптимизированный API.

*   **Конвейер обучения (Training Pipeline) и MLflow:** Автоматизированный пайплайн, который управляет жизненным циклом модели: от сбора новых данных и их разметки до обучения, валидации и регистрации новой версии модели[citation:9]. Инструменты вроде MLflow необходимы для воспроизводимости экспериментов, версионирования моделей, артефактов и данных[citation:1].

*   **Хранилище признаков (Feature Store):** Критический компонент для обеспечения согласованности (consistency) между этапами обучения и инференса[citation:9]. В нем хранятся актуальные значения признаков (например, последняя известная загрузка кухни ресторана), которые используются при онлайн-предсказании, а также исторические срезы для переобучения модели.

*   **Мониторинг (Prometheus, Grafana, Логи):** Непрерывный мониторинг необходим для надежной работы ML-системы[citation:3]. Отслеживаются как системные метрики (задержка, RPS, использование CPU), так и метрики качества модели (распределение предсказаний, расхождение с ground truth). Это позволяет быстро выявлять дрейф данных (data drift) или деградацию модели[citation:5].

### 2. Архитектура Data Pipeline (Конвейер данных)

Этот пайплайн отвечает за превращение сырых данных из различных источников в готовый набор признаков (feature set) для обучения модели и онлайн-инференса[citation:6][citation:10].

![Data Pipeline](./img2.png)

**Пояснение этапов и обоснование выбора паттернов:**

1.  **Ингestion (Сбор данных):** Данные поступают как **потоково (streaming)** в реальном времени (координаты курьеров, события заказов), так и **пакетами (batch)** (ежедневные выгрузки из систем ресторанов, исторические данные о трафике). Такой гибридный подход (иногда называемый Lambda-архитектурой) позволяет балансировать между скоростью и полнотой данных[citation:6].

2.  **Хранение сырых данных (Raw Data Storage / Data Lake):** Все сырые данные сохраняются в долговременное, дешевое хранилище (например, S3) в форматах вроде Parquet. Это создает "источник истины" для аудита, повторной обработки и отладки. Следует применять схему "озера данных" (Data Lake) с разделами Bronze (сырые данные), Silver (очищенные) и Gold (готовые для использования)[citation:2].

3.  **Разметка (Labeling):** Автоматический процесс вычисления целевой переменной — фактического времени доставки (Ground Truth) — по завершенным заказам. Этот модуль является ключевым для сбора данных для последующего переобучения модели[citation:3].

4.  **Обработка и Feature Engineering (Data Processing):** На этом этапе выполняются **ETL/ELT-процессы**[citation:6][citation:10]:
    *   **Очистка:** Обработка пропущенных значений, фильтрация выбросов.
    *   **Трансформация:** Расчет признаков: преобразование временных меток (час, день недели), вычисление расстояния по дорогам (через вызов внешнего API в офлайн-режиме), агрегация исторических данных (среднее время приготовления в ресторане за последнюю неделю).
    *   **Обогащение:** Соединение данных из разных источников (заказ + ресторан + курьер).
    Для обработки больших объемов исторических данных (batch) используется распределенный фреймворк, такой как **Apache Spark**.

5.  **Feature Store и Serving Store:** Обработанные и рассчитанные признаки сохраняются в Feature Store. Для онлайн-обслуживания используется высокопроизводительная база (например, Redis или специализированный Feature Store), которая может отдавать актуальные значения признаков с минимальной задержкой при инференсе.

### 3. Архитектура Training Pipeline (Конвейер обучения)

Этот пайплайн автоматизирует процесс создания новых версий моделей, реализуя принципы MLOps[citation:5][citation:9].

![Training Pipeline](./img3.png)

**Пояснение процесса и обоснование решений:**

1.  **Триггер (Trigger):** Пайплайн может запускаться **по расписанию** (например, каждую ночь), **по событию** (накопилось N новых размеченных заказов) или **вручную** (при изменении кода модели). Использование оркестратора вроде **Apache Airflow** позволяет управлять сложными зависимостями задач[citation:10].

2.  **Подготовка данных (Data Preparation):** Из Feature Store извлекается обучающий датасет. Обязательным этапом является **разделение данных** с учетом временного порядка (например, обучаться на данных за прошлый месяц, валидироваться на данных прошлой недели), чтобы избежать "утечки будущего" (data leakage) и адекватно оценить производительность[citation:9].

3.  **Обучение и валидация (Training & Validation):**
    *   **Обучение:** Запуск обучения модели XGBoost с логированием всех гиперпараметров, метрик и артефактов в **MLflow**.
    *   **Валидация:** Оценка модели на отложенной (hold-out) выборке или с помощью кросс-валидации во времени. Вычисление ключевых метрик регрессии: MAE (Mean Absolute Error), RMSE (Root Mean Square Error), MAPE (Mean Absolute Percentage Error). Особое внимание уделяется ошибкам на критичных сегментах (например, заказы в час пик, заказы в плохую погоду).
    *   **Сравнение с бейзлайном:** Новая модель обязательно сравнивается с текущей продакшен-моделью или простым эвристическим бейзлайном (например, историческое среднее время доставки для ресторана)[citation:3][citation:9].

4.  **Тестирование и регистрация (Testing & Registry):** Если модель проходит пороговые значения метрик, она упаковывается (в Docker-контейнер) и регистрируется в **MLflow Model Registry**. Модель получает статус `Staging`.

5.  **Развертывание (Deployment):** Развертывание новой версии модели в изолированное стейджинг-окружение для проведения окончательного **A/B-теста** или **canary-развертывания** на небольшом проценте живого трафика[citation:3]. Только после подтверждения стабильности и качества в реальных условиях модель промоутируется в статус `Production` и полностью заменяет предыдущую.

### 4. Архитектура Inference Pipeline (Сервис предсказаний)

Эта подсистема отвечает за обслуживание запросов в режиме реального времени с соблюдением требований к задержке и надежности[citation:3].

![Inference Pipeline](./img4.png)

**Пояснение компонентов и стратегий обеспечения производительности:**

*   **Балансировщик нагрузки (Load Balancer):** Распределяет входящие запросы (RPS) между репликами сервиса инференса, развернутыми в Kubernetes.

*   **Сервис инференса (Inference Service Pod):**
    1.  **Получение запроса:** REST API принимает идентификаторы заказа, ресторана, курьера.
    2.  **Извлечение признаков (Feature Fetching):** Сервис обогащает запрос, извлекая недостающие признаки из **Feature Store** (статические признаки ресторана) и **кэша** (актуальные данные о трафике). Для ускорения часто используемые или медленно меняющиеся признаки кэшируются в памяти сервиса или в **Redis**.
    3.  **Предсказание (Inference):** Применение модели XGBoost к сформированному вектору признаков. Использование формата **ONNX** может дать дополнительное ускорение инференса.
    4.  **Пост-обработка:** Возможна дополнительная корректировка предсказания на основе простых бизнес-правил (fallback-стратегия) на случай сбоя модели или получения аномального значения[citation:9].

*   **Горизонтальное автомасштабирование (HPA - Horizontal Pod Autoscaler):** Kubernetes автоматически увеличивает или уменьшает количество подов (реплик) сервиса инференса на основе метрик нагрузки (CPU, память, кастомные метрики RPS). Это критически важно для обработки пиковых нагрузок в 19,576 RPS.

*   **Canary Deployment:** Стратегия безопасного развертывания, при которой новая версия модели сначала получает лишь небольшой процент (например, 5%) живого трафика. Ее метрики (латентность, точность) тщательно сравниваются со старой версией. Это минимизирует риски при обновлении.

*   **Всесторонний мониторинг:**
    *   **Prometheus/Grafana:** Для системных метрик и бизнес-метрик (средний ETA, распределение предсказаний).
    *   **Централизованное логирование (ELK Stack):** Для анализа отдельных запросов и ошибок.
    *   **Трассировка (Tracing):** Для отслеживания цепочки вызовов и выявления узких мест в задержке, особенно при вызове внешних API (трафик).

## Часть 3: Расчёты и нефункциональные требования

### 1. Расчёт требований к хранилищу

*   **Размер записи:** ~1 КБ (фичи + метаданные заказа).
*   **Объем данных в день:** 1,417,129 DAU * 2 заказа/пользователя * 1 КБ ≈ **2.8 ГБ/день**.
*   **Сырые данные (30 дней хранения):** 2.8 ГБ/день * 30 ≈ **84 ГБ**.
*   **Обработанные данные и Feature Store (90 дней):** С учетом агрегированных признаков и истории ~5 ГБ/день * 90 ≈ **450 ГБ**.
*   **Логи и метрики:** ~10 ГБ/день * 30 ≈ **300 ГБ**.
*   **Модели и артефакты:** Незначительно (~1-5 ГБ).
*   **Итого ориентировочный объем:** **~850 ГБ - 1 ТБ**. Использование облачных объектных хранилищ (S3) и эффективных форматов (Parquet) позволяет масштабироваться и оптимизировать затраты.

### 2. Расчёт требований к пропускной способности и инфраструктуре

*   **Пиковая нагрузка:** 19,576 RPS.
*   **Производительность одного инстанса сервиса:** Современный CPU-инстанс с оптимизированной моделью XGBoost (ONNX) может обрабатывать **~1000-1500 RPS** при latency <50 мс.
*   **Количество pod в Kubernetes при пике:** 19,576 RPS / 1200 RPS/pod ≈ **17 подов**.
*   **Запас для отказоустойчивости и скейлинга:** Начальный пул в **20-25 подов** с автомасштабированием до 40-50.
*   **Пропускная способность сети:** Входящий трафик ~20 МБ/с, исходящий ~2-4 МБ/с. Требуется сетевая инфраструктура с пропускной способностью от 1 Гбит/с.

### 3. Масштабируемость и надёжность

**Масштабируемость (Scalability):**
*   **Горизонтальное масштабирование (Scale-out):** Все ключевые компоненты — Kafka, сервис инференса (K8s HPA), базы данных — спроектированы для масштабирования путем добавления новых нод.
*   **Разделение ответственности:** Микросервисная архитектура позволяет независимо масштабировать конвейер обучения, сервис инференса и системы мониторинга.

**Надёжность (Reliability) и Отказоустойчивость (Fault Tolerance):**
*   **Репликация и избыточность:** Кластеры Kafka, базы данных (Feature Store, Redis), поды в Kubernetes развертываются с несколькими репликами.
*   **Graceful Degradation (Контролируемая деградация):** При недоступности внешнего API трафика система может переключиться на использование кэшированных данных или исторических паттернов, продолжая выдавать консервативные прогнозы[citation:9].
*   **Health Checks и Self-healing:** Kubernetes постоянно проверяет "живость" (liveness) и "готовность" (readiness) подов, автоматически перезапуская упавшие.
*   **Резервное копирование и восстановление:** Регулярные бэкапы критических данных (реестр моделей MLflow, конфигурации) и возможность быстрого отката (rollback) к предыдущей стабильной версии модели через Model Registry.
*   **Проактивный мониторинг и алертинг:** Настройка алертов в Grafana на аномальные значения ключевых метрик (рост latency, скачок ошибок предсказания, падение RPS) позволяет реагировать на проблемы до того, как они повлияют на пользователей[citation:3].

## Список использованных источников

1.  Колодезев, Е. (2023). Курс по ML System Design. *Kolodezev.ru*. [https://kolodezev.ru/mlsystemdesign.html](https://kolodezev.ru/mlsystemdesign.html) [citation:1]
2.  Segner, M. (2023, 21 марта). Data Pipeline Architecture Explained: 6 Diagrams and Best Practices. *Monte Carlo Data Blog*. [https://www.montecarlodata.com/blog-data-pipeline-architecture-explained/](https://www.montecarlodata.com/blog-data-pipeline-architecture-explained/) [citation:2]
3.  Архитектура реальной системы машинного обучения. (2022, 26 июля). *Хабр / VK Cloud Solutions*. [https://habr.com/ru/companies/vk/articles/673782/](https://habr.com/ru/companies/vk/articles/673782/) [citation:3]
4.  Национальный исследовательский университет «Высшая школа экономики». (2024). Проектирование систем машинного обучения. [https://www.hse.ru/edu/courses/989476787](https://www.hse.ru/edu/courses/989476787) [citation:4]
5.  Cloud.ru. (нет даты). Курс Cloud.ru ML System Design. [https://cloud.ru/education/kurs-cloud-ru-ml-system-design](https://cloud.ru/education/kurs-cloud-ru-ml-system-design) [citation:5]
6.  Timeplus Team. (2024, 2 февраля). Data Pipeline Architecture: Components & Diagrams Explained. *Timeplus Blog*. [https://www.timeplus.com/post/data-pipeline-architecture](https://www.timeplus.com/post/data-pipeline-architecture) [citation:6]
7.  Amazon Web Services. (нет даты). Что такое построение архитектурных диаграмм? *AWS*. [https://aws.amazon.com/ru/what-is/architecture-diagramming/](https://aws.amazon.com/ru/what-is/architecture-diagramming/) [citation:7]
8.  Segner, M. (нет даты). Data Pipeline Architecture Examples And Diagrams From Real Teams. *Medium*. [https://medium.com/@michaelrsegner/data-pipeline-architecture-examples-and-diagrams-from-real-teams-659ebd62cadc](https://medium.com/@michaelrsegner/data-pipeline-architecture-examples-and-diagrams-from-real-teams-659ebd62cadc) [citation:8]
9.  Бабушкин, В. (нет даты). ML Design: Основы проектирования систем машинного обучения. *Karpov.courses*. [https://karpov.courses/ml-design](https://karpov.courses/ml-design) [citation:9]
10. Datamation. (2025, 3 октября). Data Pipeline Architecture: A Comprehensive Guide. *Datamation*. [https://www.datamation.com/big-data/data-pipeline-architecture/](https://www.datamation.com/big-data/data-pipeline-architecture/) [citation:10]
